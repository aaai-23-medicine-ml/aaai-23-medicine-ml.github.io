<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <title> AAAI-23 Bridge Program:  </title>
  Bridge between Medicine and Machine Learning.
  <meta name="robots" content="index,follow" />
  <link rel="stylesheet" type="text/css" href="fonts.css" />
  <link rel="stylesheet" type="text/css" href="style.css" />
  <script src="jquery.js"></script>
  <script src="bootstrap.min.js"></script>
</head>

<body data-spy="scroll" data-target=".navbar" id="about">
<div class='content'>

    <div class='title'>
      <a href="index.html"><strong>AAAI-23 Bridge Program: <br/></strong> 
         Bridge between Medicine and Machine Learning </a>
    </div>

    <div class="band">
    <a class="band" class="thumb"><img src=imgs/brain_mri.png></a>
    <a class="band" class="thumb"><img src=imgs/chest-x-rays.png></a>
    <a class="band" class="thumb"><img src=imgs/scanner.png></a>
    <a class="band" class="thumb"><img src=imgs/ct.png></a>
    </div>


<div class='article'>


<h2 id="schedule">Speakers </h2>

<b> Lockout: Sparse Regularization of Neural Networks
 </b> </br> 

Gilmer Valdes </br>
Associate Professor </br>
Department of Radiation Oncology University of California, San Francisco </br>
Department of Epidemiology and Biostatistics University of California, Berkeley </br> 


</br>
Abstract: </br>
Many regression and classification procedures fit a function 𝑓(𝑥;𝑤) of predictor variables 𝑥 to data {𝑥𝑖,𝑦𝑖}1𝑁 based on some loss criterion 𝐿(𝑦,𝑓(𝑥;𝑤)). Often, regularization is applied to improve accuracy by placing a constraint 𝑃(𝑤)≤𝑡 on the values of the parameters 𝑤, where 𝑃 is a monotonic increasing function of the absolute values of the parameters (e.g. Lasso, Ridge). Although efficient methods exist for finding solutions to these constrained optimization problems for all values of 𝑡≥0 in the special case when 𝑓 is a linear function, none are available when 𝑓 is non-linear (e.g. Neural Networks). Here we present a fast algorithm that provides all such solutions (path) for any differentiable function f and loss 𝐿, and any differentiable constraint 𝑃 that is an increasing monotone function of the absolute value of each parameter. Applications involving sparsity inducing regularization of arbitrary Neural Networks are discussed. Empirical results indicate that these sparse solutions are usually superior to their dense counterparts in both accuracy and interpretability (sometimes strikingly better). This improvement in accuracy can often make Neural Networks competitive with, and sometimes superior to, state-of-the-art methods in the analysis of tabular data. Specific applications to Medicine, in particular, Microarray data are discussed.
</br>
</br>
Bio: </br>

Dr Valdes is an associate professor with appointments in the Departments of Radiation Oncology , Epidemiology and Biostatistics at UCSF and in the joint program in Computer and Precision Health between UCSF and UCB. After completing his doctoral training in medical physics at UCLA, Dr Valdes completed a fellowship and clinical residency in Therapeutic Medical Physics at UPENN. His work during early training focused on development of machine learning algorithms and advanced modeling to predict patient outcomes, tailor treatments based on the type and quality of data available (particularly for prostate cancer patients), and more recently, on development of methods that permit inspection and interpretation of machine learning models (MediBoost, The Additive Tree, Expert Augmented Machine Learning, Lockout). Finally, he was fortunate to have been awarded an NI BIBK08 career award to support the development of accurate and interpretable machine learning algorithms for their application to medicine.

</div> <!-- end of div article -->

</div> <!-- end of div content -->

</body>

</html>

